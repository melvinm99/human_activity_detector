{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yidAw6JVsToz",
        "outputId": "eb4fbfaa-1b9f-4480-e1cb-64b9a3f175e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KofMA8JRiHme",
        "outputId": "dd5c32f3-f498-4197-d9ba-e65fd271233f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-12 16:33:01--  http://extrasensory.ucsd.edu/data/primary_data_files/ExtraSensory.per_uuid_features_labels.zip\n",
            "Resolving extrasensory.ucsd.edu (extrasensory.ucsd.edu)... 132.239.17.138\n",
            "Connecting to extrasensory.ucsd.edu (extrasensory.ucsd.edu)|132.239.17.138|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 225374973 (215M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 214.93M  42.6MB/s    in 5.5s    \n",
            "\n",
            "2024-10-12 16:33:07 (39.3 MB/s) - ‘data.zip’ saved [225374973/225374973]\n",
            "\n",
            "Archive:  data.zip\n",
            "  inflating: user_data/00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz  \n",
            "  inflating: user_data/098A72A5-E3E5-4F54-A152-BBDA0DF7B694.features_labels.csv.gz  \n",
            "  inflating: user_data/0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv.gz  \n",
            "  inflating: user_data/0BFC35E2-4817-4865-BFA7-764742302A2D.features_labels.csv.gz  \n",
            "  inflating: user_data/0E6184E1-90C0-48EE-B25A-F1ECB7B9714E.features_labels.csv.gz  \n",
            "  inflating: user_data/1155FF54-63D3-4AB2-9863-8385D0BD0A13.features_labels.csv.gz  \n",
            "  inflating: user_data/11B5EC4D-4133-4289-B475-4E737182A406.features_labels.csv.gz  \n",
            "  inflating: user_data/136562B6-95B2-483D-88DC-065F28409FD2.features_labels.csv.gz  \n",
            "  inflating: user_data/1538C99F-BA1E-4EFB-A949-6C7C47701B20.features_labels.csv.gz  \n",
            "  inflating: user_data/1DBB0F6F-1F81-4A50-9DF4-CD62ACFA4842.features_labels.csv.gz  \n",
            "  inflating: user_data/24E40C4C-A349-4F9F-93AB-01D00FB994AF.features_labels.csv.gz  \n",
            "  inflating: user_data/27E04243-B138-4F40-A164-F40B60165CF3.features_labels.csv.gz  \n",
            "  inflating: user_data/2C32C23E-E30C-498A-8DD2-0EFB9150A02E.features_labels.csv.gz  \n",
            "  inflating: user_data/33A85C34-CFE4-4732-9E73-0A7AC861B27A.features_labels.csv.gz  \n",
            "  inflating: user_data/3600D531-0C55-44A7-AE95-A7A38519464E.features_labels.csv.gz  \n",
            "  inflating: user_data/40E170A7-607B-4578-AF04-F021C3B0384A.features_labels.csv.gz  \n",
            "  inflating: user_data/481F4DD2-7689-43B9-A2AA-C8772227162B.features_labels.csv.gz  \n",
            "  inflating: user_data/4E98F91F-4654-42EF-B908-A3389443F2E7.features_labels.csv.gz  \n",
            "  inflating: user_data/4FC32141-E888-4BFF-8804-12559A491D8C.features_labels.csv.gz  \n",
            "  inflating: user_data/5119D0F8-FCA8-4184-A4EB-19421A40DE0D.features_labels.csv.gz  \n",
            "  inflating: user_data/5152A2DF-FAF3-4BA8-9CA9-E66B32671A53.features_labels.csv.gz  \n",
            "  inflating: user_data/59818CD2-24D7-4D32-B133-24C2FE3801E5.features_labels.csv.gz  \n",
            "  inflating: user_data/59EEFAE0-DEB0-4FFF-9250-54D2A03D0CF2.features_labels.csv.gz  \n",
            "  inflating: user_data/5EF64122-B513-46AE-BCF1-E62AAC285D2C.features_labels.csv.gz  \n",
            "  inflating: user_data/61359772-D8D8-480D-B623-7C636EAD0C81.features_labels.csv.gz  \n",
            "  inflating: user_data/61976C24-1C50-4355-9C49-AAE44A7D09F6.features_labels.csv.gz  \n",
            "  inflating: user_data/665514DE-49DC-421F-8DCB-145D0B2609AD.features_labels.csv.gz  \n",
            "  inflating: user_data/74B86067-5D4B-43CF-82CF-341B76BEA0F4.features_labels.csv.gz  \n",
            "  inflating: user_data/78A91A4E-4A51-4065-BDA7-94755F0BB3BB.features_labels.csv.gz  \n",
            "  inflating: user_data/797D145F-3858-4A7F-A7C2-A4EB721E133C.features_labels.csv.gz  \n",
            "  inflating: user_data/7CE37510-56D0-4120-A1CF-0E23351428D2.features_labels.csv.gz  \n",
            "  inflating: user_data/7D9BB102-A612-4E2A-8E22-3159752F55D8.features_labels.csv.gz  \n",
            "  inflating: user_data/8023FE1A-D3B0-4E2C-A57A-9321B7FC755F.features_labels.csv.gz  \n",
            "  inflating: user_data/806289BC-AD52-4CC1-806C-0CDB14D65EB6.features_labels.csv.gz  \n",
            "  inflating: user_data/81536B0A-8DBF-4D8A-AC24-9543E2E4C8E0.features_labels.csv.gz  \n",
            "  inflating: user_data/83CF687B-7CEC-434B-9FE8-00C3D5799BE6.features_labels.csv.gz  \n",
            "  inflating: user_data/86A4F379-B305-473D-9D83-FC7D800180EF.features_labels.csv.gz  \n",
            "  inflating: user_data/96A358A0-FFF2-4239-B93E-C7425B901B47.features_labels.csv.gz  \n",
            "  inflating: user_data/9759096F-1119-4E19-A0AD-6F16989C7E1C.features_labels.csv.gz  \n",
            "  inflating: user_data/99B204C0-DD5C-4BB7-83E8-A37281B8D769.features_labels.csv.gz  \n",
            "  inflating: user_data/9DC38D04-E82E-4F29-AB52-B476535226F2.features_labels.csv.gz  \n",
            "  inflating: user_data/A5A30F76-581E-4757-97A2-957553A2C6AA.features_labels.csv.gz  \n",
            "  inflating: user_data/A5CDF89D-02A2-4EC1-89F8-F534FDABDD96.features_labels.csv.gz  \n",
            "  inflating: user_data/A7599A50-24AE-46A6-8EA6-2576F1011D81.features_labels.csv.gz  \n",
            "  inflating: user_data/A76A5AF5-5A93-4CF2-A16E-62353BB70E8A.features_labels.csv.gz  \n",
            "  inflating: user_data/B09E373F-8A54-44C8-895B-0039390B859F.features_labels.csv.gz  \n",
            "  inflating: user_data/B7F9D634-263E-4A97-87F9-6FFB4DDCB36C.features_labels.csv.gz  \n",
            "  inflating: user_data/B9724848-C7E2-45F4-9B3F-A1F38D864495.features_labels.csv.gz  \n",
            "  inflating: user_data/BE3CA5A6-A561-4BBD-B7C9-5DF6805400FC.features_labels.csv.gz  \n",
            "  inflating: user_data/BEF6C611-50DA-4971-A040-87FB979F3FC1.features_labels.csv.gz  \n",
            "  inflating: user_data/C48CE857-A0DD-4DDB-BEA5-3A25449B2153.features_labels.csv.gz  \n",
            "  inflating: user_data/CA820D43-E5E2-42EF-9798-BE56F776370B.features_labels.csv.gz  \n",
            "  inflating: user_data/CCAF77F0-FABB-4F2F-9E24-D56AD0C5A82F.features_labels.csv.gz  \n",
            "  inflating: user_data/CDA3BBF7-6631-45E8-85BA-EEB416B32A3C.features_labels.csv.gz  \n",
            "  inflating: user_data/CF722AA9-2533-4E51-9FEB-9EAC84EE9AAC.features_labels.csv.gz  \n",
            "  inflating: user_data/D7D20E2E-FC78-405D-B346-DBD3FD8FC92B.features_labels.csv.gz  \n",
            "  inflating: user_data/E65577C1-8D5D-4F70-AF23-B3ADB9D3DBA3.features_labels.csv.gz  \n",
            "  inflating: user_data/ECECC2AB-D32F-4F90-B74C-E12A1C69BBE2.features_labels.csv.gz  \n",
            "  inflating: user_data/F50235E0-DD67-4F2A-B00B-1F31ADA998B9.features_labels.csv.gz  \n",
            "  inflating: user_data/FDAA70A1-42A3-4E3F-9AE3-3FDA412E03BF.features_labels.csv.gz  \n"
          ]
        }
      ],
      "source": [
        "!wget -O data.zip http://extrasensory.ucsd.edu/data/primary_data_files/ExtraSensory.per_uuid_features_labels.zip\n",
        "!mkdir user_data\n",
        "!unzip data.zip -d user_data\n",
        "#!cd user_data && for g in *.gz; do gunzip $g; done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "756s88yNogeq"
      },
      "outputs": [],
      "source": [
        "#!rm -rf user_data\n",
        "#!mkdir user_data\n",
        "#!unzip data.zip -d user_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE1oAEEhbfbX"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtfsTWvBbfbY"
      },
      "source": [
        "resource : https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nbsLD3FbfbY"
      },
      "source": [
        "1. Load the extrasensory dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akMbFR5VbfbY"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "import gzip\n",
        "import os\n",
        "import shutil\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OememtH1h_Uw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getinvehicle(row):\n",
        "    if row['label:ON_A_BUS'] == 1 or row['label:IN_A_CAR'] == 1 or row['label:DRIVE_-_I_M_THE_DRIVER'] == 1 or row['label:DRIVE_-_I_M_A_PASSENGER'] == 1:\n",
        "        val = 1\n",
        "    else:\n",
        "        val = 0\n",
        "    return val\n",
        "\n",
        "def dataset_fix():\n",
        "    with open('sensors.csv') as file:\n",
        "        featuresToKeep = [line.rstrip() for line in file]\n",
        "\n",
        "    with open('labels.csv') as file:\n",
        "        labelsToKeep = [line.rstrip() for line in file]\n",
        "\n",
        "    #with zipfile.ZipFile('ExtraSensory.per_uuid_features_labels.zip', 'r') as zip_ref:\n",
        "        #zip_ref.extractall('user_data')\n",
        "\n",
        "    df_list = []\n",
        "\n",
        "    for file in os.listdir('user_data'):\n",
        "        with gzip.open('user_data/'+file, 'rb') as f_in:\n",
        "            with open(f_in.name.replace('.gz', ''), 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    for file in os.listdir('user_data'):\n",
        "        if file.endswith('.csv'):\n",
        "            df = pd.read_csv('user_data/'+file)\n",
        "\n",
        "            df['activityType:in_vehicle'] = df.apply(getinvehicle, axis=1)\n",
        "\n",
        "            cols = list(df.columns)\n",
        "            features = list(filter(lambda s: not s.startswith(\"label\"), cols))\n",
        "            labels = list(filter(lambda s: s.startswith(\"label\"), cols))\n",
        "\n",
        "            toswap = [\"label:FIX_walking\", \"label:FIX_running\", \"label:BICYCLING\", \"label:SLEEPING\"]\n",
        "\n",
        "            features.extend(toswap)\n",
        "            labels = [x for x in labels if x not in set(toswap)]\n",
        "            features.extend(labels)\n",
        "\n",
        "            df = df.reindex(columns=features)\n",
        "            df.rename(columns={'label:FIX_walking': 'activityType:walking', 'label:FIX_running': 'activityType:running',\n",
        "                               'label:BICYCLING': 'activityType:on_bicycle', 'label:SLEEPING': 'activityType:sleeping'},\n",
        "                      inplace=True)\n",
        "\n",
        "\n",
        "            columnsToRemove = []\n",
        "\n",
        "            for series_name, series in df.items():\n",
        "                if series_name not in featuresToKeep and series_name not in labelsToKeep:\n",
        "                    columnsToRemove.append(series_name)\n",
        "\n",
        "            df.drop(columns=columnsToRemove, inplace=True)\n",
        "            #df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "            # Save the final result to a new CSV file\n",
        "            df.to_csv('user_data/'+file, index=False, na_rep='nan')\n",
        "\n",
        "            #f_in = open('user_data/' + file)\n",
        "            #f_out = gzip.open('user_data/' + file + 'test.gz', 'wb')\n",
        "            #f_out.writelines(f_in)\n",
        "            #f_out.close()\n",
        "            #f_in.close()\n",
        "\n",
        "            with open('user_data/' + file, 'rb') as f_in, gzip.open('user_data/' + file + '.gz', 'wb') as f_out:\n",
        "              f_out.writelines(f_in)\n",
        "\n",
        "dataset_fix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDatsxj1cI-7"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class select_features_by_name(BaseEstimator, TransformerMixin):\n",
        "    '''\n",
        "    Select feature by given feature names. Compatible with sklearn transformer class.\n",
        "\n",
        "    Keyword Arguments:\n",
        "        feature_to_use: [list] -- feature names in list we want to select\n",
        "        feature_names: [list] -- feature names of all the possible features\n",
        "    '''\n",
        "\n",
        "    def __init__(self, feature_to_use, feature_names):\n",
        "\n",
        "        self.feature_to_use = feature_to_use\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "        fi = []\n",
        "        for i, feature in enumerate(self.feature_names):\n",
        "            if feature in self.feature_to_use:\n",
        "                fi.append(i)\n",
        "        self.fi = fi\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return X[:, self.fi]\n",
        "\n",
        "class select_features_by_sensors(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    '''\n",
        "    Select feature by given sensor names. Compatible with sklearn transformer class.\n",
        "\n",
        "    Keyword Arguments:\n",
        "        sensors_to_use: [list] -- feature names in list we want to select\n",
        "        feature_names: [list] -- feature names of all the possible features\n",
        "    '''\n",
        "\n",
        "    def __init__(self, sensors_to_use, feature_names):\n",
        "        self.sensors_to_use = sensors_to_use\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        fi = []\n",
        "        for i, feature in enumerate(self.feature_names):\n",
        "            if sensor_name_abbriviation[feature.split(':')[0]] in self.sensors_to_use:\n",
        "                fi.append(i)\n",
        "\n",
        "        return X[:, fi]\n",
        "\n",
        "def select_target_labels(X, y, M, target_labels, label_names, drop_all_zero = True):\n",
        "    '''\n",
        "    Given target labels, labels matrix and all the possible label names, return label matrix with only target labels\n",
        "    '''\n",
        "    li = []\n",
        "    for i, label_name in enumerate(label_names):\n",
        "        if label_name in target_labels:\n",
        "            li.append(i)\n",
        "\n",
        "    sample_index = (np.sum(y[:, li], axis = 1) != 0)\n",
        "    if drop_all_zero:\n",
        "        return X[sample_index, :], y[sample_index, :][:, li], M[sample_index, :][:, li]\n",
        "    else:\n",
        "        return X[:,:], y[:, li], M[:, li]\n",
        "\n",
        "def split_by_users(X, y, M, test_uuid, user_index):\n",
        "    '''\n",
        "    Split the dataset into training and test set given the test users' index in uuid_list\n",
        "    '''\n",
        "    X_train, y_train, M_train = [], [], []\n",
        "    X_test, y_test, M_test = [], [], []\n",
        "\n",
        "    for i in range(60):\n",
        "        if i in test_uuid:\n",
        "            X_test.append(X[user_index[i]:user_index[i+1], :])\n",
        "            M_test.append(M[user_index[i]:user_index[i+1], :])\n",
        "            y_test.append(y[user_index[i]:user_index[i+1]])\n",
        "        else:\n",
        "            X_train.append(X[user_index[i]:user_index[i+1], :])\n",
        "            M_train.append(M[user_index[i]:user_index[i+1], :])\n",
        "            y_train.append(y[user_index[i]:user_index[i+1]])\n",
        "\n",
        "    X_train = np.concatenate(X_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "    M_train = np.concatenate(M_train)\n",
        "    X_test = np.concatenate(X_test)\n",
        "    y_test = np.concatenate(y_test)\n",
        "    M_test = np.concatenate(M_test)\n",
        "\n",
        "    return X_train, y_train, M_train, X_test, y_test, M_test\n",
        "\n",
        "\n",
        "if __name__ == 'main':\n",
        "\n",
        "    None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20Adf3TxcrfM"
      },
      "outputs": [],
      "source": [
        "\n",
        "class feature_selector():\n",
        "\n",
        "\tdef __init__(self, feature_names):\n",
        "\n",
        "\t\tself.feature_names = feature_names\n",
        "\t\tself.selected_features = []\n",
        "\n",
        "\tdef forward_sequential_selection(self, X_train, y_train, X_test, y_test, model,\n",
        "\t\t\t\t\t\t\t\t\t\tevaluation = 'BA', report = 'True'):\n",
        "\t\t'''\n",
        "\t\tTake splitted dataset and a model, use forward sequential selection to select a\n",
        "\t\tsubset of features that gives the highest score on given test set.\n",
        "\n",
        "\t\tKeyword Arguments:\n",
        "\t\t\tX_train, y_train, X_test, y_test: [narray] -- splitted dataset\n",
        "\t\t\tmodel: [sklearn model] -- model used to fit and predict the dataset\n",
        "\t\t\tevaluation: [str] -- specify the type of evaluation (score)\n",
        "\t\t\treport: [Boolen] -- whether to report the progress\n",
        "\n",
        "\t\t'''\n",
        "\n",
        "\t\tself.n_labels = y_train.shape[-1]\n",
        "\t\tself.evaluation = evaluation\n",
        "\n",
        "\t\tselected_features = self.selected_features\n",
        "\t\tremaining_features = self.feature_names.copy()\n",
        "\t\tfor feature in selected_features:\n",
        "\t\t\tremaining_features.remove(feature)\n",
        "\t\tscore = 0\n",
        "\n",
        "\t\tfor i in range(len(remaining_features)):\n",
        "\t\t\tflag = 0\n",
        "\n",
        "\t\t\t# Try all the features that has not been selected\n",
        "\t\t\tfor j, feature in enumerate(remaining_features):\n",
        "\n",
        "\t        \t# Select features\n",
        "\t\t\t\ttemp_features = selected_features + [feature]\n",
        "\t\t\t\tfeature_selector = select_features_by_name(temp_features, self.feature_names)\n",
        "\t\t\t\ttemp_X_train = feature_selector.fit_transform(X_train)\n",
        "\t\t\t\ttemp_X_test = feature_selector.transform(X_test)\n",
        "\n",
        "\t\t\t\t# Fit the model\n",
        "\t\t\t\tmodel.fit(temp_X_train, y_train)\n",
        "\n",
        "\t\t\t\t# predict and calcuate score\n",
        "\t\t\t\tif evaluation == 'model_default':\n",
        "\t\t\t\t\ttemp_score = model.score(temp_X_test, y_test)\n",
        "\t\t\t\telif evaluation == 'BA':\n",
        "\t\t\t\t\t_, _, _, temp_score = evaluate_model(model, temp_X_test, y_test, report = False)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\traise NameError('Evaluation does not exist!')\n",
        "\n",
        "\t\t\t\t# If the score increases, update\n",
        "\t\t\t\tif temp_score > score:\n",
        "\n",
        "\t\t\t\t\tscore = temp_score\n",
        "\t\t\t\t\tadded_feature = feature\n",
        "\t\t\t\t\tflag = 1\n",
        "\t\t\t\tprint('Try feature: %s\\t[%d/%d]\\t score: %f' %(feature, j, len(remaining_features), temp_score))\n",
        "\n",
        "\t\t\tif flag == 0:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t        # Update current selected features and remaining feature to be selected\n",
        "\t\t\tselected_features = selected_features + [added_feature]\n",
        "\t\t\tremaining_features.remove(added_feature)\n",
        "\n",
        "\t\t\tif report:\n",
        "\t\t\t\tprint('\\nAdd %s\\t%d features selected\\tscore %f' %(added_feature, len(selected_features), score))\n",
        "\n",
        "\t\tself.selected_features = selected_features\n",
        "\n",
        "\t\treturn selected_features, score\n",
        "\n",
        "if __name__ == 'main':\n",
        "\n",
        "\tNone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy9LN7Mnbv06",
        "outputId": "774e608d-ae12-4855-c033-ae3e73177937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 60 users data.\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import io as StringIO\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def parse_header_of_csv(csv_str):\n",
        "    # Isolate the headline columns:\n",
        "    headline = csv_str[:csv_str.index('\\n')]\n",
        "    columns = headline.split(',')\n",
        "\n",
        "    # The first column should be timestamp:\n",
        "    assert columns[0] == 'timestamp'\n",
        "    # The last column should be label_source:\n",
        "    #assert columns[-1] == 'label_source'\n",
        "\n",
        "    # Search for the column of the first label:\n",
        "    for (ci,col) in enumerate(columns):\n",
        "        if col.startswith('label:'):\n",
        "            first_label_ind = ci\n",
        "            break\n",
        "        pass\n",
        "\n",
        "    # Feature columns come after timestamp and before the labels:\n",
        "    feature_names = columns[1:first_label_ind]\n",
        "    # Then come the labels, till the one-before-last column:\n",
        "    label_names = columns[first_label_ind:-1]\n",
        "    for (li,label) in enumerate(label_names):\n",
        "        # In the CSV the label names appear with prefix 'label:', but we don't need it after reading the data:\n",
        "        assert label.startswith('label:')\n",
        "        label_names[li] = label.replace('label:','')\n",
        "        pass\n",
        "\n",
        "    return (feature_names,label_names)\n",
        "\n",
        "def parse_body_of_csv(csv_str,n_features):\n",
        "    # Read the entire CSV body into a single numeric matrix:\n",
        "    full_table = np.loadtxt(StringIO.StringIO(csv_str),delimiter=',',skiprows=1)\n",
        "    # Timestamp is the primary key for the records (examples):\n",
        "    timestamps = full_table[:,0].astype(int)\n",
        "\n",
        "    # Read the sensor features:\n",
        "    X = full_table[:,1:(n_features+1)]\n",
        "\n",
        "    # Read the binary label values, and the 'missing label' indicators:\n",
        "    trinary_labels_mat = full_table[:,(n_features+1):-1] # This should have values of either 0., 1. or NaN\n",
        "    M = np.isnan(trinary_labels_mat) # M is the missing label matrix\n",
        "    Y = np.where(M,0,trinary_labels_mat) > 0. # Y is the label matrix\n",
        "\n",
        "    return (X,Y,M,timestamps)\n",
        "\n",
        "'''\n",
        "Read the data (precomputed sensor-features and labels) for a user.\n",
        "This function assumes the user's data file is present.\n",
        "'''\n",
        "def read_user_data(uuid):\n",
        "    user_data_file = 'user_data/%s.features_labels.csv' % uuid\n",
        "\n",
        "    # Read the entire csv file of the user:\n",
        "    #with gzip.open(user_data_file,'rb') as fid:\n",
        "        #csv_str = fid.read().decode()\n",
        "        #pass\n",
        "\n",
        "    #user_data_file = 'user_data/%s.features_labels.csv' % uuid\n",
        "    #with open(user_data_file,'rb') as file:\n",
        "      #csv_str = file.read()\n",
        "      #pass\n",
        "\n",
        "    with open(user_data_file, 'r') as csvfile:\n",
        "      separator = ''\n",
        "      csv_str = separator.join(csvfile.readlines())\n",
        "\n",
        "    (feature_names,label_names) = parse_header_of_csv(csv_str)\n",
        "    n_features = len(feature_names)\n",
        "    (X,Y,M,timestamps) = parse_body_of_csv(csv_str,n_features)\n",
        "\n",
        "    return (X,Y,M,timestamps,feature_names,label_names)\n",
        "\n",
        "\n",
        "'''\n",
        "Load the data of all the users\n",
        "'''\n",
        "\n",
        "UUID_LIST = []\n",
        "\n",
        "for file in os.listdir('user_data'):\n",
        "    if file.split('.')[-1] == 'gz':\n",
        "        UUID_LIST.append(file.split('.')[0])\n",
        "print('Found %d users data.' %(len(UUID_LIST)))\n",
        "\n",
        "def load_all_data(uuid_list = UUID_LIST):\n",
        "    '''\n",
        "    Load data from all the sixty users\n",
        "\n",
        "    Returns:\n",
        "        X: [narray] -- feature matrix in shape of [n_smaples, n_features]\n",
        "        y: [narray] -- label matrix in shape of [n_smaples, n_labels]\n",
        "        M: [narray] -- missing label matrix, element is false means that the label is missing\n",
        "        user_index: [list] -- indicate the starting index of each user data\n",
        "        feature_names: [list] -- feature names of all the possible features\n",
        "        label_names: [list] -- label names of all the possible labels\n",
        "    '''\n",
        "\n",
        "    X, y, M = [], [], []\n",
        "    user_index = [0]\n",
        "\n",
        "    for i, uuid in enumerate(uuid_list):\n",
        "\n",
        "        X_i,y_i,M_i,timestamps,feature_names,label_names = read_user_data(uuid)\n",
        "        user_index.append(user_index[i]+X_i.shape[0])\n",
        "\n",
        "        X.append(X_i)\n",
        "        y.append(y_i)\n",
        "        M.append(M_i)\n",
        "\n",
        "    X = np.concatenate(X)\n",
        "    y = np.concatenate(y)\n",
        "    M = np.concatenate(M)\n",
        "\n",
        "    return X, y, M, user_index, feature_names, label_names\n",
        "\n",
        "#if __name__ == 'main':\n",
        "\n",
        "    # load the data of the first user in the list\n",
        "    #X,Y,M,timestamps,feature_names,label_names = read_user_data(UUID_LIST[0])\n",
        "\n",
        "    # Load the data of all the users\n",
        "    #X, y, M, user_index, feature_names, label_names = load_all_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTRObPLjbfbZ"
      },
      "outputs": [],
      "source": [
        "def score_function(y_test, y_pred, score = 'BA', W_test = None):\n",
        "\n",
        "    mcm = []\n",
        "    for i in range(y_test.shape[1]):\n",
        "        if W_test is not None:\n",
        "            cm = confusion_matrix(y_test[:,i].T, y_pred[:,i].T, sample_weight = W_test[:,i].T)\n",
        "        else:\n",
        "            cm = confusion_matrix(y_test[:,i].T, y_pred[:,i].T)\n",
        "        cm = np.expand_dims(cm, axis = 0)\n",
        "        mcm.append(cm)\n",
        "\n",
        "    mcm = np.concatenate(mcm, axis = 0)\n",
        "    tn = mcm[:, 0, 0]\n",
        "    tp = mcm[:, 1, 1]\n",
        "    fn = mcm[:, 1, 0]\n",
        "    fp = mcm[:, 0, 1]\n",
        "\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    BA = (sensitivity + specificity)/2\n",
        "    accuracy = (tn + tp)/(tn + tp + fn + fp)\n",
        "\n",
        "    sensitivity = np.sum(sensitivity)/sensitivity.shape[0]\n",
        "    specificity = np.sum(specificity)/specificity.shape[0]\n",
        "    BA = np.sum(BA)/BA.shape[0]\n",
        "    accuracy = np.sum(accuracy)/accuracy.shape[0]\n",
        "\n",
        "    if score == 'BA':\n",
        "        return BA\n",
        "    else:\n",
        "        raise Exception('score not valid!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvqsaH7cbfbZ"
      },
      "outputs": [],
      "source": [
        "# Load all the data from Extrasenory dataset\n",
        "X, y, M, user_index, feature_names, label_names = load_all_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhAaA_cj4JSY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd5CjNhBbfbZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Only select body state label\n",
        "#target_label = ['LYING_DOWN', 'SITTING', 'FIX_walking', 'FIX_running', 'BICYCLING', 'OR_standing']\n",
        "target_label = [\"SITTING\"]\n",
        "# Use the last 5 user's data as test set\n",
        "#test_uuid = list(range(56, 61))\n",
        "\n",
        "# Fill the Nan with mean value and normalize all the data\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy = 'mean')),\n",
        "    ('std_scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Transform\n",
        "# 1. select target labels\n",
        "# 2. tansform feature matrix fill None with mean and do the normalization\n",
        "# 3. Split train, validation and test set by ratio of 6:2:2\n",
        "#X_new, y_new, M_new = select_target_labels(X175, y, M, target_label, label_names, drop_all_zero = False)\n",
        "#X_new, y_new, M_new = select_target_labels(X, y, M, target_label, label_names, drop_all_zero = False)\n",
        "#X_new = pipeline.fit_transform(X_new, y_new)\n",
        "#type(X_new)\n",
        "#X_train, y_train, M_train, X_val, y_val, M_val, X_test, y_test, M_test = random_split(X_new, y_new, M_new, test_size = 0.2, val_size = 0.2, random_seed = 42)\n",
        "#X_train, y_train, M_train, X_val, y_val, M_val, X_test, y_test, M_test = random_split(X_new, y_new, M_new, test_size = 0.2, val_size = 0.2, random_seed = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUDZZO9tkxPy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy = 'mean')),\n",
        "    ('std_scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "X_new = pipeline.fit_transform(X, y)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size= 0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2-Z5EYzbfba"
      },
      "source": [
        "# LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuHsgQU1bfba"
      },
      "outputs": [],
      "source": [
        "# [samples, time_steps, features]\n",
        "X_train_1 = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_1 = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW1iztzgbfba",
        "outputId": "fd28a18f-3f35-46be-de3e-c13438797d9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m633s\u001b[0m 2ms/step - accuracy: 0.0952 - loss: 0.1306 - val_accuracy: 0.0773 - val_loss: 0.0738\n",
            "Epoch 2/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 2ms/step - accuracy: 0.0779 - loss: 0.0742 - val_accuracy: 0.0773 - val_loss: 0.0726\n",
            "Epoch 3/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 2ms/step - accuracy: 0.0774 - loss: 0.0734 - val_accuracy: 0.0773 - val_loss: 0.0722\n",
            "Epoch 4/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 2ms/step - accuracy: 0.0783 - loss: 0.0736 - val_accuracy: 0.0773 - val_loss: 0.0719\n",
            "Epoch 5/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 2ms/step - accuracy: 0.0781 - loss: 0.0734 - val_accuracy: 0.0773 - val_loss: 0.0718\n",
            "Epoch 6/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 2ms/step - accuracy: 0.0776 - loss: 0.0725 - val_accuracy: 0.0773 - val_loss: 0.0716\n",
            "Epoch 7/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m674s\u001b[0m 2ms/step - accuracy: 0.0782 - loss: 0.0728 - val_accuracy: 0.0773 - val_loss: 0.0715\n",
            "Epoch 8/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 2ms/step - accuracy: 0.0769 - loss: 0.0732 - val_accuracy: 0.0773 - val_loss: 0.0715\n",
            "Epoch 9/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 2ms/step - accuracy: 0.0781 - loss: 0.0730 - val_accuracy: 0.0773 - val_loss: 0.0714\n",
            "Epoch 10/10\n",
            "\u001b[1m301876/301876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 2ms/step - accuracy: 0.0779 - loss: 0.0730 - val_accuracy: 0.0773 - val_loss: 0.0714\n",
            "save 1 error\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, Input\n",
        "\n",
        "hidden_size = 1\n",
        "time_step = 1\n",
        "\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2.add(Input(batch_shape = (hidden_size, time_step, X_train_1.shape[2])))\n",
        "model_2.add(LSTM(1, activation = \"sigmoid\"))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(Dense(14, activation = \"sigmoid\"))\n",
        "model_2.compile(loss = \"binary_crossentropy\",\n",
        "              optimizer = 'sgd',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# fit network\n",
        "model_lstm = model_2.fit(X_train_1, y_train,\n",
        "                         epochs = 10,\n",
        "                         batch_size = 1,\n",
        "                         validation_data = (X_test_1, y_test))\n",
        "                         #sample_weight = y_train_sw1D)\n",
        "\n",
        "\n",
        "try:\n",
        "  model_2.save(\"/drive/MyDrive/Tesi/model_2.keras\")\n",
        "except:\n",
        "  print(\"save 1 error\")\n",
        "\n",
        "\n",
        "try:\n",
        "  model_2.save(\"/content/drive/MyDrive/Tesi/model_2.keras\")\n",
        "except:\n",
        "  print(\"save 2 error\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test predict modello"
      ],
      "metadata": {
        "id": "PbRhSrJCo8ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XixoxRGfsRBp"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "  model_2.save(\"/model.keras\")\n",
        "except:\n",
        "  print(\"save 2 error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsSGEGaIbfba"
      },
      "outputs": [],
      "source": [
        "print(model_lstm.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_Yj-3j_bfba"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy\n",
        "#plt.plot(score_rnn)\n",
        "list_lstm = model_lstm.history['val_accuracy'] + [0.6942]\n",
        "#list_rnn = model.history[\"acc\"]\n",
        "\n",
        "fig = plt.figure(figsize = (16, 10))\n",
        "#plt.plot(model.history['acc'])\n",
        "plt.plot(list[0])\n",
        "plt.ylabel('Balanced Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7n3zCTEbfba"
      },
      "outputs": [],
      "source": [
        "#plt.plot(model_rnn.history['val_acc'], \"RNN\")\n",
        "plt.plot(model_lstm.history['val_accuracy'])\n",
        "plt.title(\"Accuarcy Comparison\")\n",
        "plt.ylabel('Balanced Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['RNN', 'RNN with LSTM'], loc = 'upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "15gpbErBbfbZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}